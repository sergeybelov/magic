{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import hstack\n",
    "import datetime as dt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 35, 40\n",
    "%matplotlib inline\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "import matplotlib.cm as cm\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import __version__\n",
    "from plotly import graph_objs as go\n",
    "from plotly.graph_objs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle('MG_Sales_customer.pickle',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "#выбираем покупателей для дальнейшего анализа\n",
    "sales_sum=df.groupby('Покупатель')['Количество'].sum().sort_values()\n",
    "#выкидываем со слишком большими продажами (сводные карты) и тех кто купил один раз\n",
    "sales_sum.drop(sales_sum[(sales_sum>sales_sum.tail(6)[0])].index, inplace=True)#|(sales_sum==1)\n",
    "customers_name=list(sales_sum.index)\n",
    "del sales_sum\n",
    "\n",
    "#делаем выборку\n",
    "select=df.loc[(df['Покупатель'].isin(customers_name))&(df['Дата']>=(dt.datetime(2016,1,1))),['Покупатель','ПокупательПол','ПокупательДатаРождения','ВидИзделия','ПодвидИзделия','СтильДизайна','ВидДизайна','ОсновнойКамень','ГруппаТовара','Коллекция','ЦветМеталла','ТоварСреднийВес','Размер','Вес','Количество','Сумма','Дата','ПокупательОтказОтСМС']]\n",
    "del customers_name\n",
    "del df\n",
    "\n",
    "#Подготовка датасета\n",
    "#ЦветМеталла=list(map(lambda xx: xx,list(select['ЦветМеталла'].unique())))\n",
    "def codeMetall(_str):    \n",
    "    for str_split in _str.lower().split():\n",
    "        if str_split=='серебро': return 0\n",
    "        if str_split=='золото': return 10\n",
    "        if str_split=='зол.': return 11\n",
    "        if str_split=='платина': return 20\n",
    "        if str_split=='сплав': return -10\n",
    "    return -20\n",
    "\n",
    "\n",
    "select['ПокупательПолКод']=select['ПокупательПол'].map(lambda xx: {'Ж':0, 'М':1, '<Неопределено>':None}[xx])\n",
    "select['ЦветМеталлаКод']=select['ЦветМеталла'].map(lambda xx: codeMetall(xx))\n",
    "select['ПокупательПолКод'].fillna(select['ПокупательПолКод'].median(),inplace=True)\n",
    "select['ПокупательГодРождения']=select['ПокупательДатаРождения'].dt.year\n",
    "select['ПокупательГодРождения']=select['ПокупательГодРождения'].map(lambda xx: None if xx<1917 else xx)\n",
    "select['ПокупательГодРождения']=select['ПокупательГодРождения'].map(lambda xx: None if xx>2010 else xx)\n",
    "#select['ПокупательГодРождения'].fillna(select['ПокупательГодРождения'].median(),inplace=True)\n",
    "select.drop(['ПокупательДатаРождения','ЦветМеталла','ПокупательПолКод'],  axis=1, inplace=True)\n",
    "#выборка колонок\n",
    "numerical_columns = [c for c in select.columns if select[c].dtype.name != 'object']\n",
    "categorial_columns = [c for c in select.columns if select[c].dtype.name == 'object']\n",
    "\n",
    "numerical_columns=list(set(numerical_columns)-(set(['ПокупательГодРождения','Дата','Сумма'])))\n",
    "categorial_columns=list(set(categorial_columns)-(set(['ПокупательПол','ПокупательОтказОтСМС'])))\n",
    "\n",
    "\n",
    "\n",
    "#Dummy-кодирование и шкалируем\n",
    "lb_style = LabelBinarizer(sparse_output=True)\n",
    "concList=[]\n",
    "for col in categorial_columns:\n",
    "    concList.append(lb_style.fit_transform(select[col]))    \n",
    "concList.append(StandardScaler().fit_transform(select[numerical_columns]))#добавляем шклированные значения числовых переменных\n",
    "X=hstack(concList)\n",
    "\n",
    "del concList\n",
    "print('shape ',X.shape)\n",
    "print('Prepare finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=int(math.sqrt(X.shape[1])), n_iter=5)\n",
    "svd_representation = svd.fit_transform(X)\n",
    "var1=np.cumsum(np.round(svd.explained_variance_ratio_, decimals=5)*100)\n",
    "#plt.plot(var1[-50:])\n",
    "\n",
    "#расчитываем оптимальное количество компонент\n",
    "#более 90% дисперсии и шаг приращения каждой следующей компоненты <10^-4\n",
    "optimal_n=np.intersect1d(np.argwhere(var1>90.),np.argwhere(svd.explained_variance_ratio_<=10**-4))[0]\n",
    "print(optimal_n)#171\n",
    "\n",
    "if optimal_n==None:\n",
    "    raise 'Not enough n_components!'\n",
    "\n",
    "svd = TruncatedSVD(n_components=optimal_n, n_iter=7)\n",
    "svd_representation = svd.fit_transform(X)\n",
    "print('reduced')\n",
    "del var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_size=svd_representation.shape[0]\n",
    "#k=.05#n_clusters = 3\n",
    "#k=.01#n_clusters = 30\n",
    "#k=.05#n_clusters = 6\n",
    "#k=.03#n_clusters = 6\n",
    "#k=.009#n_clusters = 33\n",
    "#k=.01#n_clusters = 33\n",
    "#k=.02#n_clusters = 33\n",
    "#k=.03#n_clusters = 6 \n",
    "#k=.02#n_clusters = 8\n",
    "k=.025#n_clusters = 6 \n",
    "\n",
    "k=.025#n_clusters = 4\n",
    "k=.01#n_clusters = 12\n",
    "k=.02#n_clusters = 8\n",
    "\n",
    "k= 0.02#n_clusters = 3\n",
    "k= 0.01#n_clusters = 15\n",
    "k= 0.015#n_clusters = 4\n",
    "k= 0.012#n_clusters = \n",
    "\n",
    "print('k=',k)\n",
    "hdb_t1 = time.time()\n",
    "#hdb = MiniBatchKMeans(n_clusters=n_clusters,max_iter=150,max_no_improvement=15,batch_size=165,tol=.01,n_init=5,random_state=17).fit(svd_representation)\n",
    "#hdb = MiniBatchKMeans(n_clusters=n_clusters,max_iter=150,max_no_improvement=15,batch_size=260,tol=.0094,n_init=5,random_state=17).fit(svd_representation)\n",
    "hdb = HDBSCAN(min_cluster_size=int(sel_size*k),metric='manhattan',core_dist_n_jobs=8).fit(svd_representation)#,metric='manhattan' alpha=.00001,alpha=.1,min_samples=1\n",
    "\n",
    "\n",
    "hdb_elapsed_time = time.time() - hdb_t1\n",
    "print('HDBSCAN Elapsed time to cluster: %.1f m' % (hdb_elapsed_time/60))\n",
    "\n",
    "n_clusters = len(set(hdb.labels_)) - (1 if -1 in hdb.labels_ else 0)\n",
    "print(\"n_clusters =\", n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "#svd_representation=X.copy()\n",
    "\n",
    "ss = ShuffleSplit(n_splits=1, train_size=int(sel_size*.3))\n",
    "subs= ss.split(svd_representation)#,hdb.labels_\n",
    "\n",
    "\n",
    "for index in subs:\n",
    "        X=svd_representation[index[0]]#.tocsr()\n",
    "    #for _min_cluster_size in range(100, 700, 50):\n",
    "        #print('min_cluster_size=',_min_cluster_size)\n",
    "        #hdb_t1 = time.time()\n",
    "        #hdb = HDBSCAN(min_cluster_size=_min_cluster_size,core_dist_n_jobs=8,metric='manhattan').fit(X)#,min_samples=1,alpha=.0001\n",
    "\n",
    "        #hdb_elapsed_time = time.time() - hdb_t1\n",
    "        #print('HDBSCAN Elapsed time to cluster: %.1f m' % (hdb_elapsed_time/60))\n",
    "\n",
    "        n_clusters = len(set(hdb.labels_)) - (1 if -1 in hdb.labels_ else 0)\n",
    "        print(\"n_clusters =\", n_clusters)\n",
    "\n",
    "        cluster_labels=hdb.labels_[index[0]]\n",
    "\n",
    "        hdb_t1 = time.time()\n",
    "\n",
    "        fig, (ax1) = plt.subplots(1, 1)\n",
    "        fig.set_size_inches(12, 8)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 \n",
    "        ax1.set_xlim([-.5, .5])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 12])\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print('Elapsed time to cluster: %6.1f m' % ((time.time()-hdb_t1)/60))\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "                      \"The average silhouette_score is :\", silhouette_avg)\n",
    "        \n",
    "        if silhouette_avg<=0: continue\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "        n_len=len(sample_silhouette_values[sample_silhouette_values<0])\n",
    "        one_per=len(sample_silhouette_values)/100\n",
    "        percent=round(n_len/one_per,2)\n",
    "        print('Negative silhouette count: ',n_len,', ',percent,'%')\n",
    "        #if percent>=2: raise Exception('Плохой результат силуэта при кластеризации')\n",
    "\n",
    "        y_lower = 12\n",
    "        for i in range(n_clusters):\n",
    "                    # Aggregate the silhouette scores for samples belonging to\n",
    "                    # cluster i, and sort them\n",
    "                    ith_cluster_silhouette_values = \\\n",
    "                        sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "                    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "                    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "                    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "                    color = cm.spectral(float(i) / n_clusters)\n",
    "                    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                                      0, ith_cluster_silhouette_values,\n",
    "                                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "                    # Label the silhouette plots with their cluster numbers at the middle\n",
    "                    ax1.text(-0.5, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "                    # Compute the new y_lower for next plot\n",
    "                    y_lower = y_upper + 12  # 10 for the 0 samples\n",
    "\n",
    "        set_xlabel=\"The avg silhouette coefficient values = \"+str(round(silhouette_avg,3))\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(set_xlabel)\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks(list(np.arange(-.6,1,.2)))\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for HDBSCAN clustering on sample data \"\n",
    "                              \"with n_clusters = %d\" % n_clusters),\n",
    "                             fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        #if n_clusters<=6: break\n",
    "\n",
    "del X\n",
    "del cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select['ЦветМеталлаКод']=select['ЦветМеталлаКод'].map(lambda xx: {0: 'серебро', 10: 'золото', 11: 'золото', 20: 'платина',-10: 'сплав', -20: 'прочее'}[xx])\n",
    "#select['ПокупательПолКод']=select['ПокупательПолКод'].map(lambda xx: {0: 'Ж', 1: 'М'}[xx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma=','\n",
    "\n",
    "#Выводим данные по кластерам в сводную таблицу\n",
    "clusters_data=pd.DataFrame(columns=select.columns)\n",
    "    \n",
    "for cl in range(n_clusters):\n",
    "    tsel=tsel=select.loc[hdb.labels_==cl]#выборка по кластеру\n",
    "    sel=tsel.describe(include='all')\n",
    "    clust_info=sel.loc[['top','mean']]\n",
    "    \n",
    "    \n",
    "    cat_lbls=list(clust_info.loc['top',pd.notnull(clust_info.loc['top'])].index)#категориальные переменные\n",
    "    #Рассчитываем три максимальных значения\n",
    "    for col in cat_lbls:\n",
    "        if col=='ПокупательПол': continue\n",
    "        \n",
    "        if col=='ПодвидИзделия':\n",
    "            str_vals=[]            \n",
    "            for val in clust_info.loc['top','ВидИзделия'].split(','):\n",
    "                str_vals.append(tsel.loc[tsel['ВидИзделия']==val].groupby(col)['Покупатель'].count().sort_values(ascending=False).head(1).index.values[0])\n",
    "        else:\n",
    "            str_vals=list(tsel.groupby(col)['Покупатель'].count().sort_values(ascending=False).head(3).index)\n",
    "            \n",
    "        #clust_info.loc['top',col]=str_vals.translate(trantab)\n",
    "        try:\n",
    "            clust_info.loc['top',col]=comma.join(str_vals)\n",
    "        except:\n",
    "            clust_info.loc['top',col]=''\n",
    "            \n",
    "    \n",
    "    clusters_data.loc[cl+1]=pd.concat([clust_info.fillna('').sum(axis=0),clust_info.fillna(0).sum(axis=0)])\n",
    "    \n",
    "    sex_count=tsel.groupby('ПокупательПол')['Покупатель'].nunique()\n",
    "    perc1=sex_count.sum()/100\n",
    "    clusters_data.loc[cl+1,'М (%)']=round(sex_count['М']/perc1,2)\n",
    "    clusters_data.loc[cl+1,'Ж (%)']=round(sex_count['Ж']/perc1,2)\n",
    "    \n",
    "    sms_count=tsel.groupby('ПокупательОтказОтСМС')['Покупатель'].nunique()\n",
    "    perc1=sms_count.sum()/100\n",
    "    clusters_data.loc[cl+1,'Согласные с рассылкой СМС всего (%)']=round(sms_count[False]/perc1,2)\n",
    "    \n",
    "    sales_count=tsel.groupby('Покупатель')['Дата'].nunique()#Покупатели по датам    \n",
    "    cust_names=sales_count[sales_count>1].index.values#Имена покупателей совершивших более одной покупки\n",
    "    sms_count=tsel.loc[(tsel['Покупатель'].isin(cust_names))].groupby('ПокупательОтказОтСМС')['Покупатель'].nunique()\n",
    "    perc1=sms_count.sum()/100\n",
    "    clusters_data.loc[cl+1,'Согласные с рассылкой СМС при повторных покупках (%)']=round(sms_count[False]/perc1,2)\n",
    "    \n",
    "    #clusters_data.loc[cl+1,'Количество']=sel.loc['count','Покупатель']\n",
    "    #clusters_data.loc[cl+1,'РазбросСумма']=sel.loc['std','Сумма']\n",
    "    \n",
    "    clusters_data.loc[cl+1,'Средний возраст (лет)']=dt.datetime.now().year-int(clusters_data.loc[cl+1,'ПокупательГодРождения'])\n",
    "    \n",
    "    clusters_data.loc[cl+1,'Уникальных покупателей в кластере']=sel.loc['unique','Покупатель']\n",
    "    sales_count=tsel.groupby('Покупатель')['Количество'].sum()\n",
    "    clusters_data.loc[cl+1,'Кол-во купленных изделий среднее']=sales_count.mean()\n",
    "    \n",
    "    sales_count=tsel.groupby('Покупатель')['Дата'].nunique()#Покупатели по датам\n",
    "    clusters_data.loc[cl+1,'Число покупок среднее']=sales_count.mean()\n",
    "    clusters_data.loc[cl+1,'2 Покупки']=sales_count[sales_count==2].shape[0]\n",
    "    clusters_data.loc[cl+1,'3 Покупки']=sales_count[sales_count==3].shape[0]\n",
    "    clusters_data.loc[cl+1,'4 Покупки']=sales_count[sales_count==4].shape[0]\n",
    "    clusters_data.loc[cl+1,'5 Покупок']=sales_count[sales_count==5].shape[0]\n",
    "    clusters_data.loc[cl+1,'>5 Покупок']=sales_count[sales_count>5].shape[0]\n",
    "    \n",
    "    \n",
    "    sales_count_sum=tsel.groupby('Покупатель')['Сумма'].sum()\n",
    "    clusters_data.loc[cl+1,'Средний чек']=(sales_count_sum/sales_count).mean()\n",
    "    clusters_data.loc[cl+1,'Цена изделия средняя']=sel.loc['50%','Сумма'] \n",
    "    \n",
    "    clusters_data.loc[cl+1,'Размер среднее']=clusters_data.loc[cl+1,'Размер']\n",
    "    clusters_data.loc[cl+1,'Вес среднее']=clusters_data.loc[cl+1,'Вес']\n",
    "    \n",
    "    \n",
    "clusters_data.drop(['ПокупательГодРождения','Количество','ПокупательОтказОтСМС','ПокупательПол','Сумма','ТоварСреднийВес','Размер','Вес','Дата'],axis=1,inplace=True)\n",
    "#clusters_data.sort_values('Уникальных покупателей в кластере',inplace=True, ascending=False)\n",
    "\n",
    "#clusters_data=clusters_data.reset_index()\n",
    "#clusters_data.drop('index', axis=1, inplace=True)\n",
    "clusters_data.index = range(1,clusters_data.shape[0]+1)\n",
    "\n",
    "for col in cat_lbls:\n",
    "    if col=='Покупатель':\n",
    "        clusters_data=clusters_data.rename(columns={col: col+' - основные в кластере'})\n",
    "    else:\n",
    "        clusters_data=clusters_data.rename(columns={col: col+' - 3 самых популярных'})\n",
    "    \n",
    "    \n",
    "    \n",
    "writer = pd.ExcelWriter('Кластера по продажам.xlsx')\n",
    "clusters_data.to_excel(writer,'Sheet1',index_label='№ кластера')\n",
    "writer.save()\n",
    "\n",
    "clusters_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select.loc[hdb_labels==0].groupby('ВидИзделия')['Количество'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_count=select.groupby('ПокупательОтказОтСМС')['Покупатель'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(600, 4000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_data.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
